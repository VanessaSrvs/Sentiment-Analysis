{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim \n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import Bidirectional, LSTM, Activation\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig_id):\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    plt.savefig(os.getcwd(), format=\"png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_with_tokens.csv\")\n",
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bag of Words Counts</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    \n",
    "    embedding = count_vectorizer.fit_transform(data)\n",
    "    \n",
    "    return embedding, count_vectorizer\n",
    "\n",
    "X = data[\"text\"].tolist()\n",
    "y = data[\"stars\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_counts, count_vectorizer = count_vectorize(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1255410x389905 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 98667654 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load word2vec model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_WORD2VEC = \"../../GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(PATH_WORD2VEC, binary=True,limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Overview of the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 492840 unique tokens.\n",
      "embedding_weights shape:  (492841, 300)\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in data[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in data[\"tokens\"]]\n",
    "vocabulary = sorted(list(set(all_words)))\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(data[\"text\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(data[\"text\"].tolist())\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % (len(word_index)))\n",
    "\n",
    "train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(data[\"stars\"]))\n",
    "\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\n",
    "\n",
    "embedding_weights = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    embedding_weights[index, :] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "print(\"embedding_weights shape: \", embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bidir_LSTM_2layer(embeddings, max_sequence_length, num_words, \\\n",
    "                embedding_dim, trainable=False):\n",
    "    \"\"\"\n",
    "    Function creating the 2-layer Bidirectional LSTM classifier's model graph\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating the embedding layer pretrained with word2vec vectors\n",
    "    embedding_layer = Embedding(num_words, \\\n",
    "                                embedding_dim, \\\n",
    "                                weights=[embeddings], \\\n",
    "                                input_length=max_sequence_length, \\\n",
    "                                trainable=trainable)\n",
    "    \n",
    "    # define input of the graph\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), dtype=\"int32\", name=\"Input\")\n",
    "    \n",
    "    # Propagate sequence_input through the embedding layer, to get back the embeddings\n",
    "    embedded_sequences = embedding_layer(sequence_input)    \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True, name=\"LSTM_1\"))(embedded_sequences)\n",
    "    # Add dropout with probability 0.5\n",
    "    X = Dropout(0.5, name=\"Dropout_1\")(X)\n",
    "    # Propagate X through another LSTM layer with 128-dimensional hidden state\n",
    "    X = Bidirectional(LSTM(128, return_sequences=False, name=\"LSTM_2\"))(X)\n",
    "    # Add dropout with probability 0.5\n",
    "    X = Dropout(0.5, name=\"Dropout_2\")(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(6, name=\"Dense\")(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation(\"softmax\", name=\"Softmax\")(X)\n",
    "    \n",
    "    # Create Model instance which converts sequence_input into X\n",
    "    model = Model(inputs=sequence_input, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "X_val  = train_data[-num_validation_samples:]\n",
    "y_val  = labels[-num_validation_samples:]\n",
    "\n",
    "model = Bidir_LSTM_2layer(embedding_weights, \\\n",
    "                          MAX_SEQUENCE_LENGTH, \\\n",
    "                          len(word_index)+1, \\\n",
    "                          EMBEDDING_DIM, \\\n",
    "                          trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35, 300)           147852300 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 35, 256)           439296    \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 35, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 6)                 1542      \n",
      "_________________________________________________________________\n",
      "Softmax (Activation)         (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 148,687,378\n",
      "Trainable params: 835,078\n",
      "Non-trainable params: 147,852,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training the model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using categorical_crossentropy loss, adam optimizer and ['accuracy'] metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \\\n",
    "              optimizer='adam', \\\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create checkpoint that saves the weights each time validation set at each epoch is outperformed by the last one\n",
    "filepath=\"LSTM2layer_weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \\\n",
    "                             monitor=\"val_acc\", \\\n",
    "                             verbose=0, \\\n",
    "                             save_best_only=True, \\\n",
    "                             mode=\"max\")\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1255411 samples, validate on 313852 samples\n",
      "Epoch 1/10\n",
      "1255411/1255411 [==============================] - 3862s 3ms/step - loss: 1.0747 - acc: 0.5305 - val_loss: 0.9975 - val_acc: 0.5627\n",
      "Epoch 2/10\n",
      "1255411/1255411 [==============================] - 3345s 3ms/step - loss: 0.9796 - acc: 0.5708 - val_loss: 0.9516 - val_acc: 0.5829\n",
      "Epoch 3/10\n",
      "1255411/1255411 [==============================] - 2426s 2ms/step - loss: 0.9452 - acc: 0.5865 - val_loss: 0.9323 - val_acc: 0.5905\n",
      "Epoch 4/10\n",
      "1255411/1255411 [==============================] - 2308s 2ms/step - loss: 0.9236 - acc: 0.5954 - val_loss: 0.9244 - val_acc: 0.5959\n",
      "Epoch 5/10\n",
      "1255411/1255411 [==============================] - 2310s 2ms/step - loss: 0.9088 - acc: 0.6015 - val_loss: 0.9149 - val_acc: 0.5992\n",
      "Epoch 6/10\n",
      "1255411/1255411 [==============================] - 2424s 2ms/step - loss: 0.8958 - acc: 0.6064 - val_loss: 0.9085 - val_acc: 0.6015\n",
      "Epoch 7/10\n",
      "  50688/1255411 [>.............................] - ETA: 33:42 - loss: 0.8790 - acc: 0.6136"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, \\\n",
    "                    validation_data=(X_val, y_val), \\\n",
    "                    epochs=10, \\\n",
    "                    batch_size=512, \\\n",
    "                    callbacks=callbacks_list, \\\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model_bidirlstm2layer.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# Serialize weights to HDF5\n",
    "\n",
    "model.save_weights(\"model_bidirlstm2layer.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plotting the training and validation loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "train_loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "num_epochs = 5\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss_values, 'r', label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, 'b', label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "save_fig(\"Bidirec_LSTM_2layer_train&valid_loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "train_acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "num_epochs = 5\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.plot(epochs, train_acc_values, 'r', label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc_values, 'b', label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "save_fig(\"Bidirec_LSTM_2layer_train&valid_acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluate model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "test_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = to_categorical(y_test)\n",
    "\n",
    "indices = np.arange(test_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "test_data = test_data[indices]\n",
    "labels_test = labels_test[indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=test_data\n",
    "y_test=labels_test\n",
    "np.save(\"X_test\",X_test)\n",
    "np.save(\"y_test\",y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate model directly without loading json and hdf5 files\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"%s on test data: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
