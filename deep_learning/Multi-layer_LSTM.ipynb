{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import LSTM, Activation\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_fig(fig_id):\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    plt.savefig(os.getcwd(), format=\"png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_data = '../yelp_academic_dataset_review.pickle'\n",
    "data = pd.read_pickle(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing all ('\\n') characters using list comprehensions\n",
    "data['text'] = [txt.replace('\\n', '') for txt in data['text']]\n",
    "\n",
    "# Taking only text and stars columns\n",
    "data = data.loc[:, ['text', 'stars']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"tokens\"] = data.apply(lambda row: word_tokenize(row[\"text\"]), axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bag of Words Counts</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_vectorize(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    \n",
    "    embedding = count_vectorizer.fit_transform(data)\n",
    "    \n",
    "    return embedding, count_vectorizer\n",
    "\n",
    "X = data[\"text\"].tolist()\n",
    "y = data[\"stars\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_counts, count_vectorizer = count_vectorize(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load word2vec model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_WORD2VEC = \"../GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(PATH_WORD2VEC, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Overview of the model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/gabriel-hurtado/Sentiment-Analysis/blob/DeepLSTM/deep_learning/images/deepLSTM_overview.png\"\n",
    "style = \"width:700px; height:400px;\"> <br> <center>A 2-layer LSTM sequence classifier.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = [word for tokens in data[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in data[\"tokens\"]]\n",
    "vocabulary = sorted(list(set(all_words)))\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(data[\"text\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(data[\"text\"].tolist())\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % (len(word_index)))\n",
    "\n",
    "train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(data[\"stars\"]))\n",
    "\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\n",
    "\n",
    "embedding_weights = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    embedding_weights[index, :] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "print(\"embedding_weights shape: \", embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_2layer(embeddings, max_sequence_length, num_words, \\\n",
    "                embedding_dim, trainable=False):\n",
    "    \"\"\"\n",
    "    Function creating the 2-layer LSTM classifier's model graph\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating the embedding layer pretrained with word2vec vectors\n",
    "    embedding_layer = Embedding(num_words, \\\n",
    "                                embedding_dim, \\\n",
    "                                weights=[embeddings], \\\n",
    "                                input_length=max_sequence_length, \\\n",
    "                                trainable=trainable)\n",
    "    \n",
    "    # define input of the graph\n",
    "    sequence_input = Input(shape=(max_sequence_length, ), dtype=\"int32\", name=\"Input\")\n",
    "    \n",
    "    # Propagate sequence_input through the embedding layer, to get back the embeddings\n",
    "    embedded_sequences = embedding_layer(sequence_input)    \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    X = LSTM(128, return_sequences=True, name=\"LSTM_1\")(embedded_sequences)\n",
    "    # Add dropout with probability 0.5\n",
    "    X = Dropout(0.5, name=\"Dropout_1\")(X)\n",
    "    # Propagate X through another LSTM layer with 128-dimensional hidden state\n",
    "    X = LSTM(128, return_sequences=False, name=\"LSTM_2\")(X)\n",
    "    # Add dropout with probability 0.5\n",
    "    X = Dropout(0.5, name=\"Dropout_2\")(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(6, name=\"Dense\")(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation(\"softmax\", name=\"Softmax\")(X)\n",
    "    \n",
    "    # Create Model instance which converts sequence_input into X\n",
    "    model = Model(inputs=sequence_input, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "X_val  = train_data[-num_validation_samples:]\n",
    "y_val  = labels[-num_validation_samples:]\n",
    "\n",
    "model = LSTM_2layer(embedding_weights, \\\n",
    "                    MAX_SEQUENCE_LENGTH, \\\n",
    "                    len(word_index)+1, \\\n",
    "                    EMBEDDING_DIM, \\\n",
    "                    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training the model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using categorical_crossentropy loss, adam optimizer and ['accuracy'] metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \\\n",
    "              optimizer='adam', \\\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create checkpoint that saves the weights each time validation set at each epoch is outperformed by the last one\n",
    "filepath=\"LSTM2layer_weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \\\n",
    "                             monitor=\"val_acc\", \\\n",
    "                             verbose=0, \\\n",
    "                             save_best_only=True, \\\n",
    "                             mode=\"max\")\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \\\n",
    "                    validation_data=(X_val, y_val), \\\n",
    "                    epochs=5, \\\n",
    "                    batch_size=128, \\\n",
    "                    callbacks=callbacks_list, \\\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model_lstm2layer.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# Serialize weights to HDF5\n",
    "\n",
    "model.save_weights(\"model_lstm2layer.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plotting the training and validation loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "train_loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "num_epochs = 5\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss_values, 'r', label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, 'b', label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "save_fig(\"LSTM_2layer_train&valid_loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "train_acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "num_epochs = 5\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.plot(epochs, train_acc_values, 'r', label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc_values, 'b', label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "save_fig(\"LSTM_2layer_train&valid_acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluate model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Evaluate model directly without loading json and hdf5 files\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"%s on test data: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
